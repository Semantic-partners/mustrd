= MustRD Documentation
:toc: left
:toclevels: 3

== Introduction

MustRD is a framework for testing SPARQL queries against RDF datasets. It supports embedded RDFLib and external triplestores like Anzo and GraphDB. This documentation provides a comprehensive guide to using MustRD effectively.

== Getting Started

=== Project Setup

Instantiate the project https://github.com/Semantic-partners/mustrd-template[mustrd-template]. It contains the basic configuration to get MustRD running.

Clone the created repo locally.

=== Initial Configuration

If you just want to use embedded RDFLib, you don't need to change anything.

If you want to use an external triplestore such as Anzo or GraphDB, you have to tell MustRD how to connect.

The default triplestore config file is https://github.com/Semantic-partners/mustrd-template/blob/main/test/triplestore_config/triplestores.ttl[here].

You can store your passwords in another https://github.com/Semantic-partners/mustrd-template/blob/main/test/triplestore_config/triplestores_secrets.ttl[file] at the same location and same name suffixed with "_secrets".

== Configuration

=== Triple Store Configuration

For example, passwords of triplestores configured in `/test/triplestore_config/triplestores.ttl` have to be stored in the file `/test/triplestore_config/triplestores_secrets.ttl`.

By default, `.gitignore` is configured so you don't commit the secrets files, while the triplestore configuration files will be committed. Be sure to store your secrets in the separated `_secrets` file.

To associate a triplestore configuration and its secrets, use the same subject in the configuration and secret file, like it is done in the example.

Triplestore configuration can be placed anywhere, inside or outside of your project, but it has to be referenced in the test configuration.

All the triplestores defined must contain a `url` and a `port`.

Depending on the triplestore, other properties can be mandatory as well:

For Anzo:

* `gqeURI` is necessary to identify on which AnzoGraph the queries will be executed.
* `inputGraph` and `outputGraph` are also mandatory because graphs cannot be created simply on insert in Anzo. You have to create a graphmart, layers in it, and activate the graphmart. We cannot, for the moment, automatically create a testing graphmart and layers.

For GraphDB:

* Only `repository` is mandatory. If `inputGraph` is not defined, then all graphs of the repository will be queried.

=== Test Configuration

In the template project, the default test configuration is located https://github.com/Semantic-partners/mustrd-template/blob/main/test/mustrd_configuration.ttl[here].

In that file you can define different test that mustrd will run when you give that file as parameter of mustrd pytest plugin

For each test, you can configure :

* `hasSpecPath`: Folder where your mustrd spec are (looks in subfolders as well).
* `hasDataPath`: Folder where your input/output data is
* `hasPytestPath `: Path in the test hierarchy
* `triplestoreSpecPath`: Where your external triplestore configuration file is located
* `filterOnTripleStore`: List of triplestore defined in the triplestore configuration file (or RdfLib) that has to be used to run that test.

For example:

```
:test_anzo a :MustrdTest;
            :hasSpecPath "test/test_examples/";
            :hasDataPath "test/test_data";
            :hasPytestPath "anzo";
            :triplestoreSpecPath "test/triplestore_config/triplestores.ttl";
            :filterOnTripleStore triplestore:anzo_test .
```

This will run the tests contained in all the specs located in `[project_root]/test/test_examples/`, using the data located in `[project_root]/test/test_data`, against the triplestore defined as `triplestore:anzo_test` in the file `[project_root]/test/triplestore_config/triplestores.ttl`.

If needed, the secrets have to be located in `[project_root]/test/triplestore_config/triplestores_secrets.ttl`

== Run the tests

Once you have defined your triplestore(s) and your mustrd test(s), you can already run the mustrd-template's default test specs to be sure you configured everything well.

For that, open a command line and go to your repo's root folder.

Then execute the following command:

```
poetry run pytest --mustrd --config=test/mustrd_configuration.ttl --md=render/github_job_summary.md
```

which means that you use poetry to run pytest, using the mustrd pytest plugin.

To the mustrd pytest plugin, you give the parameter:

* config: Mustrd test config that you configured in <<Edit Test configuration>> 

* md: Where test execution summary will output

Now that you could run the default tests, you probably want to define your own tests!

== Creating your own Test Specifications

If you have got this far then you are probably ready to create your own specifications to test your application SPARQL queries. These will be executed against the default RDFLib triplestore unless you configure one or more alternatives. The instructions for this are included in <<Edit the triple store configuration>>.

=== Paths
All paths are consired relative. That way mustrd tests can be versionned and shared easily.
To get absolute path from relative path in a spec file, we prefix it with the first existing result in:

. Path where the spec is located
. spec_path defined in mustrd test configuration files or cmd line argument
. data_path defined in mustrd test configuration files or cmd line argument
. Mustrd folder: In case of default resources packaged with mustrd source (will be in venv when mustrd is called as library)
We intentionally use the same method to build paths in all spec components to avoid confusion.

=== Givens
These are used to specify the dataset against which the SPARQL statement will be run.
They can be generated from external sources such as an existing graph, or a file or folder containing serialised RDF. It is also possible to specify the dataset as reified RDF directly in the test step. Currently tabular data sources such as csv files or TableDatasets are not supported.
Multiple given statements can be supplied and data is combined into a single dataset for the test.

* *InheritedDataset* - This is where no data is specified but the existing data in the target graph is retained rather than being replaced with a defined set. This can be used to chain tests together or to perform checks on application data.
----
    must:given [ a must:InheritedDataset ] ;
----
* *FileDataset* - The dataset is a local file containing serialised RDF. The formats supported are the same as those for the RDFLib Graph().parse function i.e. Turtle (.ttl), NTriples (.nt), N3 (.n3), RDF/XML (.xml) and TriX. The data is used to replace any existing content in the target graph for the test.
----
    must:given [ a must:FileDataset ;
                 must:file "test/data/given.ttl" . ] ;
----
* *FolderDataset* - Very similar to the file dataset except that the location of the file is passed to the test specification as an argument from the caller. i.e. the -g option on the command line.
----
    must:given [ a must:FolderDataset ;
                 must:fileName "given.ttl" ] ;
----
* *StatementsDataset* - The dataset is defined within the test in the form of reified RDF statements. e.g.
----
    must:given [ a must:StatementsDataset ;
                 must:hasStatement [ a rdf:Statement ;
                                     rdf:subject   test-data:sub ;
                                     rdf:predicate test-data:pred ;
                                     rdf:object    test-data:obj ; ] ; ] ;
----
* *AnzoGraphmartDataset* - The dataset is contained in an Anzo graphmart and needs to be retrieved from there. The Anzo instance containing the dataset needs to be indicated in the configuration file as documented in <<Edit the triple store configuration>>.
----
    must:given [ a must:AnzoGraphmartDataset ;
                 must:graphmart "http://cambridgesemantics.com/Graphmart/43445aeadf674e09818c81cf7049e46a";
                 must:layer "http://cambridgesemantics.com/Layer/33b97531d7e148748b75e4e3c6bbf164";
    ] .
----
=== Whens
These are the actual SPARQL queries that you wish to test. Queries can be supplied as a string directly in the test or as a file containing the query. Only single When statements are currently supported.
Mustrd does not derive the query type from the actual query, so it is necessary to provide this in the specification. Supported query types are SelectSparql, ConstructSparql and UpdateSparql.

* *TextSparqlSource* - The SPARQL query is included in the test as a (multiline) string value for the property queryText.
e.g.
----
    must:when  [ a must:TextSparqlSource ;
                 must:queryText "SELECT ?s ?p ?o WHERE { ?s ?p ?o }" ;
                 must:queryType must:SelectSparql ] ;
----

* *FileSparqlSource* - The SPARQL query is contained in a local file.
e.g.
----
    must:when  [ a must:FileSparqlSource  ;
                 must:file "test/data/construct.rq" ;
                 must:queryType must:ConstructSparql  ; ] ;
----
* *FolderSparqlSource* - Similar to the file SPARQL source except that the location of the file is passed to the test specification as an argument from the caller. i.e. the -w option on the command line.
----
    must:when  [ a must:FolderSparqlSource ;
                 must:fileName "construct.rq" ;
                 must:queryType must:ConstructSparql  ; ] ;
----
* *AnzoQueryBuilderDataset* - The query is saved in the Query Builder of an Anzo instance and needs to be retrieved from there. The Anzo instance containing the dataset needs to be indicated in the configuration file as documented in <<Edit the triple store configuration>>.
----
   must:when  [ a must:AnzoQueryBuilderDataset ;
                must:queryFolder "Mustrd";
                must:queryName "mustrd-construct" ;
                must:queryType must:ConstructSparql
    ];
----
=== Thens
Then clauses are used to specify the expected result dataset for the test. These datasets can be specified in the same way as <<Givens>> except that an extended set of dataset types is supported. For the tabular results of SELECT queries TabularDatasets are required and again can be in file format such as CSV, or an inline table within the specification.
* *FileDataset* - The dataset is a local file containing serialised RDF or tabular data. The formats supported are the same as those for the RDFLib Graph().parse function i.e. Turtle (.ttl), NTriples (.nt), N3 (.n3), RDF/XML (.xml) and TriX, as well as tabular formats (.csv, .xls, .xlsx).
----
    must:then  [ a must:FileDataset ;
                 must:file "test/data/thenSuccess.xlsx" ] .
----
----
    must:then  [ a must:FileDataset ;
                 must:file "test/data/thenSuccess.nt" ] .
----
* *FolderDataset* - Very similar to the file dataset except that the location of the file is passed to the test specification as an argument from the caller. i.e. the -t option on the command line.
----
    must:then [ a must:FolderDataset ;
                 must:fileName "then.ttl" ] ;
----
* *StatementsDataset* - The dataset is defined within the test in the form of reified RDF statements e.g.
----
    must:then [ a must:StatementsDataset ;
                 must:hasStatement [ a rdf:Statement ;
                                     rdf:subject   test-data:sub ;
                                     rdf:predicate test-data:pred ;
                                     rdf:object    test-data:obj ; ] ; ] ;
----
* *TableDataset* - The contents of the table defined in RDF syntax within the specification.
E.g. a table dataset consisting of a single row and three columns.
----
    must:then  [ a must:TableDataset ;
                   must:hasRow [ must:hasBinding[
                        must:variable "s" ;
                        must:boundValue  test-data:sub ; ],
                      [ must:variable "p" ;
                        must:boundValue  test-data:pred ; ],
                      [ must:variable "o" ;
                        must:boundValue  test-data:obj ; ] ;
               ] ; ] .
----
* *OrderedTableDataset* -  This is an extension of the TableDataset which allows the row order of the dataset to be specified using the SHACL order property to support the ORDER BY clause in SPARQL SELECT queries
E.g. A table dataset consisting of two ordered rows and three columns.
----
    must:then  [ a must:OrderedTableDataset ;
                 must:hasRow [ sh:order 1 ;
                             must:hasBinding[ must:variable "s" ;
                                        must:boundValue  test-data:sub1 ; ],
                                      [ must:variable "p" ;
                                        must:boundValue  test-data:pred1 ; ],
                                      [ must:variable "o" ;
                                        must:boundValue  test-data:obj1 ; ] ; ] ,
                            [ sh:order 2 ;
                             must:hasBinding[ must:variable "s" ;
                                        must:boundValue  test-data:sub2 ; ],
                                      [ must:variable "p" ;
                                        must:boundValue  test-data:pred2 ; ],
                                      [ must:variable "o" ;
                                        must:boundValue  test-data:obj2 ; ] ; ] ;
               ] .
----
* *EmptyTable* - This is used to indicate that we are expecting an empty result from a SPARQL SELECT query.
----
    must:then  [ a must:EmptyTable ] .
----
* *EmptyGraph* - Similar to EmptyTable but used to indicate that we are expecting an empty graph as a result from a SPARQL query.
----
    must:then  [ a must:EmptyGraph ] .
----
* *AnzoGraphmartDataset* - The dataset is contained in an Anzo graphmart and needs to be retrieved from there. The Anzo instance containing the dataset needs to be indicated in the configuration file as documented in <<Edit the triple store configuration>>.
----
    must:then [ a must:AnzoGraphmartDataset ;
                must:graphmart "http://cambridgesemantics.com/Graphmart/43445aeadf674e09818c81cf7049e46a";
                must:layer "http://cambridgesemantics.com/Layer/33b97531d7e148748b75e4e3c6bbf164";
        ] .
----